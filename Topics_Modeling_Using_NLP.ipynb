{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Step-by-Step Explanation of the Code\n",
        "\n",
        "### Step 1: Load Data\n",
        "\n",
        "The code begins by loading your CSV data into a Pandas DataFrame. This DataFrame is used for further analysis.\n",
        "\n",
        "### Step 2: Preprocess Text\n",
        "\n",
        "The `preprocess_text` function is defined to preprocess the text data in the 'Content' column of the DataFrame. It tokenizes the text, converts words to lowercase, removes punctuation, and eliminates common English stop words. The preprocessed text is then joined into a single string.\n",
        "\n",
        "### Step 3: Create TF-IDF Matrix\n",
        "\n",
        "A `TfidfVectorizer` is created to convert the preprocessed text data into numerical features using the TF-IDF (Term Frequency-Inverse Document Frequency) transformation. This vectorizer is fitted to the text data, resulting in a TF-IDF matrix.\n",
        "\n",
        "### Step 4: Build an LDA Model\n",
        "\n",
        "A Latent Dirichlet Allocation (LDA) model is created using `LatentDirichletAllocation` from scikit-learn. It is fitted to the TF-IDF matrix to identify latent topics within the text data.\n",
        "\n",
        "### Step 5: Retrieve Top N Words and Weights for Each Topic\n",
        "\n",
        "The code defines `N`, which represents the number of top words to retrieve for each topic. Then, it retrieves the terms (words) associated with each topic from the TF-IDF vectorizer and the weights of the topics from the LDA model.\n",
        "\n",
        "### Step 6: Create DataFrame\n",
        "\n",
        "A DataFrame named `df_topics` is created to store the topics and their corresponding weights. The code enters a loop to process each topic:\n",
        "- It identifies the top N word indices for the topic with `top_word_indices`.\n",
        "- It retrieves the actual top words based on the indices and stores them in `top_words`.\n",
        "- The maximum weight of the topic is obtained and stored in `max_weight`.\n",
        "- The topic words and max weight are added to the `topic_data` list for each topic.\n",
        "\n",
        "### Step 7: Save to CSV\n",
        "\n",
        "Finally, the code saves the topics and weights in the `df_topics` DataFrame to a CSV file named 'topics_weights.csv'. The resulting CSV file contains two columns: 'topic' and 'weight,' where 'topic' contains a list of the top words for each topic, and 'weight' contains the maximum weight associated with each topic.\n",
        "\n",
        "This code allows you to analyze the dominant topics in your text data and store them in a structured CSV format for further examination or visualization.\n"
      ],
      "metadata": {
        "id": "lm1_iVpXqkzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "import numpy as np\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "\n",
        "# Load your CSV data into a DataFrame\n",
        "data = pd.read_csv('WhatsappChat.csv')\n",
        "\n",
        "# Preprocess the text data\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word.lower() for word in tokens if word.isalpha()]\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    return \" \".join(tokens)  # Join tokens into a single string\n",
        "\n",
        "data['Content'] = data['Content'].apply(preprocess_text)\n",
        "\n",
        "# Create a TfidfVectorizer to convert text data into numerical features\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(data['Content'])\n",
        "\n",
        "# Build an LDA model\n",
        "lda_model = LatentDirichletAllocation(n_components=100, random_state=42)\n",
        "lda_model.fit(tfidf_matrix)\n",
        "\n",
        "# Get the top N topic words and weights for each topic\n",
        "N = 5  # Change N to the desired number of top words per topic\n",
        "topic_terms = tfidf_vectorizer.get_feature_names_out()\n",
        "topic_weights = lda_model.components_\n",
        "# for topic in topic_terms:\n",
        "#   print(topic)\n",
        "# Create a DataFrame for topics and weights\n",
        "topic_data = []\n",
        "for i, topic_weight in enumerate(topic_weights):\n",
        "    top_word_indices = topic_weight.argsort()[:-N-1:-1]  # Get top word indices\n",
        "    top_words = [topic_terms[idx] for idx in top_word_indices]  # Get top words\n",
        "    max_weight = topic_weight.max()  # Get max weight\n",
        "    topic_data.append((top_words, max_weight))\n",
        "\n",
        "df_topics = pd.DataFrame(topic_data, columns=['topic', 'weight'])\n",
        "\n",
        "# Save topics and weights to a CSV file\n",
        "df_topics.to_csv('topics_weights.csv', index=False)\n",
        "# print(topic_data)"
      ],
      "metadata": {
        "id": "D6-mpH09O96_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This below code For analysing data or topic more detaily. Above written code improved version of below code. So you use above code for topic modeling."
      ],
      "metadata": {
        "id": "AMVjQ1HFsGUA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52V3k2a0FVUh",
        "outputId": "ffb54968-a083-4e5d-972f-87dd24bc06b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvKrN_-fFoBV",
        "outputId": "6c65b9be-ede6-4426-b4b3-c028cf546f06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2tKhyAw-anV",
        "outputId": "fe05797f-216f-4017-dab4-e625d5d12714"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0, '0.019*\"good\" + 0.013*\"would\" + 0.009*\"morning\" + 0.007*\"thing\" + 0.007*\"well\" + 0.007*\"thanks\" + 0.007*\"love\" + 0.007*\"help\" + 0.007*\"send\" + 0.007*\"please\"')\n",
            "(1, '0.029*\"https\" + 0.017*\"ok\" + 0.014*\"yes\" + 0.014*\"data\" + 0.012*\"work\" + 0.010*\"analysis\" + 0.010*\"open\" + 0.007*\"see\" + 0.006*\"model\" + 0.005*\"pls\"')\n",
            "(2, '0.063*\"media\" + 0.062*\"omitted\" + 0.017*\"power\" + 0.016*\"bi\" + 0.015*\"data\" + 0.007*\"tableau\" + 0.006*\"year\" + 0.006*\"workshop\" + 0.005*\"guys\" + 0.005*\"know\"')\n",
            "(3, '0.015*\"data\" + 0.009*\"see\" + 0.009*\"guys\" + 0.009*\"mda\" + 0.008*\"interesting\" + 0.008*\"power\" + 0.008*\"work\" + 0.007*\"also\" + 0.007*\"excel\" + 0.007*\"collaboration\"')\n",
            "(4, '0.033*\"okay\" + 0.020*\"thanks\" + 0.014*\"mda\" + 0.011*\"boss\" + 0.009*\"use\" + 0.009*\"extract\" + 0.008*\"one\" + 0.008*\"work\" + 0.007*\"try\" + 0.007*\"please\"')\n",
            "(5, '0.028*\"data\" + 0.012*\"analyst\" + 0.010*\"work\" + 0.009*\"job\" + 0.009*\"one\" + 0.009*\"want\" + 0.008*\"still\" + 0.008*\"welcome\" + 0.008*\"working\" + 0.008*\"expenditure\"')\n",
            "(6, '0.035*\"thanks\" + 0.014*\"bro\" + 0.012*\"mda\" + 0.011*\"capital\" + 0.010*\"code\" + 0.009*\"data\" + 0.008*\"yeah\" + 0.008*\"think\" + 0.007*\"alright\" + 0.007*\"well\"')\n",
            "(7, '0.023*\"thank\" + 0.019*\"please\" + 0.014*\"come\" + 0.013*\"one\" + 0.012*\"good\" + 0.012*\"na\" + 0.012*\"get\" + 0.011*\"still\" + 0.010*\"message\" + 0.009*\"u\"')\n",
            "(8, '0.013*\"nice\" + 0.013*\"need\" + 0.012*\"like\" + 0.010*\"know\" + 0.010*\"go\" + 0.009*\"people\" + 0.008*\"chart\" + 0.007*\"data\" + 0.007*\"see\" + 0.007*\"expense\"')\n",
            "(9, '0.013*\"link\" + 0.013*\"data\" + 0.012*\"see\" + 0.012*\"us\" + 0.009*\"done\" + 0.009*\"great\" + 0.008*\"work\" + 0.008*\"business\" + 0.007*\"also\" + 0.007*\"one\"')\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "import numpy as np\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "\n",
        "# Load your CSV data into a DataFrame\n",
        "data = pd.read_csv('WhatsappChat.csv')\n",
        "\n",
        "# Preprocess the text data\n",
        "def preprocess_text(text):\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove punctuation and convert to lowercase\n",
        "    tokens = [word.lower() for word in tokens if word.isalpha()]\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "# Apply preprocessing to the 'Content' column\n",
        "data['Content'] = data['Content'].apply(preprocess_text)\n",
        "\n",
        "# Create a dictionary and a corpus for topic modeling\n",
        "dictionary = corpora.Dictionary(data['Content'])\n",
        "corpus = [dictionary.doc2bow(text) for text in data['Content']]\n",
        "\n",
        "# Build an LDA model\n",
        "lda_model = gensim.models.LdaModel(corpus, num_topics=10, id2word=dictionary, passes=15)\n",
        "\n",
        "# Extract and print topics\n",
        "topics = lda_model.print_topics()\n",
        "for topic in topics:\n",
        "    print(topic)\n",
        "\n",
        "\n",
        "# Save topics to 'topics.txt'\n",
        "with open('topics.txt', 'w') as file:\n",
        "    for topic in topics:\n",
        "        file.write(f'Topic {topic[0]}: {topic[1]}\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lm9qMaSzKCQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d_vrTJ2gL8Ny"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}